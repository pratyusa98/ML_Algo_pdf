{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  Bag_Of_Words(Count Vectorizer) :-\n",
    "\n",
    "- The Bag of Words (BoW) model is the simplest form of text representation in numbers. Like the term itself, we can represent a sentence as a bag of words vector (a string of numbers).\n",
    "\n",
    "- Let’s recall the three types of movie reviews we saw earlier:\n",
    "\n",
    "        Review 1: This movie is very scary and long\n",
    "        Review 2: This movie is not scary and is slow\n",
    "        Review 3: This movie is spooky and good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We will first build a vocabulary from all the unique words in the above three reviews. The vocabulary consists of these 11 words: ‘This’, ‘movie’, ‘is’, ‘very’, ‘scary’, ‘and’, ‘long’, ‘not’,  ‘slow’, ‘spooky’,  ‘good’.\n",
    "\n",
    "- We can now take each of these words and mark their occurrence in the three movie reviews above with 1s and 0s. This will give us 3 vectors for 3 reviews:\n",
    "\n",
    "<img src=\"2.webp\">\n",
    "\n",
    "Vector of Review 1: [1 1 1 1 1 1 1 0 0 0 0]\n",
    "\n",
    "Vector of Review 2: [1 1 2 0 0 1 1 0 1 0 0]\n",
    "\n",
    "Vector of Review 3: [1 1 1 0 0 0 1 0 0 1 1]\n",
    "\n",
    "And that’s the core idea behind a Bag of Words (BoW) model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "paragraph =  \"\"\"  This movie is very scary and long.\n",
    "                 This movie is not scary and is slow.\n",
    "                 This movie is spooky and good.\n",
    "                 \"\"\"\n",
    "               \n",
    "               \n",
    "# Cleaning the texts\n",
    "import re\n",
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "corpus = []\n",
    "for i in range(len(sentences)):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', sentences[i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)\n",
    "\n",
    "    \n",
    "# Creating the Bag of Words model\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_features = 1500)\n",
    "X = cv.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this movie is very scary and long', 'this movie is not scary and is slow', 'this movie is spooky and good']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1],\n",
       "       [1, 0, 2, 0, 1, 1, 1, 1, 0, 1, 0],\n",
       "       [1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0]], dtype=int64)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(corpus)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here I Am not any preprocessing tool as i wish to show what exactly BOW doing. But in real word you must doing all the preprocess step then you will be apply the BOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drawbacks of using a Bag-of-Words (BoW) Model\n",
    "In the above example, we can have vectors of length 11. However, we start facing issues when we come across new sentences:\n",
    "\n",
    "- If the new sentences contain new words, then our vocabulary size would increase and thereby, the length of the vectors would increase too.\n",
    "- Additionally, the vectors would also contain many 0s, thereby resulting in a sparse matrix (which is what we would like to avoid)\n",
    "- We are retaining no information on the grammar of the sentences nor on the ordering of the words in the text.\n",
    "- To Over Come this we use Term Frequency-Inverse Document Frequency (TF-IDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "- “Term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus.”\n",
    "\n",
    "### TF-IDF Vectorizer :\n",
    "\n",
    "- TF-IDF stands for term frequency-inverse document frequency. It highlights a specific issue which might not be too frequent in our corpus but holds great importance. The TF–IFD value increases proportionally to the number of times a word appears in the document and decreases with the number of documents in the corpus that contain the word.\n",
    "- It is composed of 2 sub-parts, which are :\n",
    "\n",
    "            1. Term Frequency (TF)\n",
    "            2. Inverse Document Frequency (IDF)\n",
    "\n",
    "#### 1. Term Frequency:-\n",
    "\n",
    "- Term frequency specifies how frequently a term appears in the entire document.\n",
    "                        Formula - TF =  Number of repetation word in sentence / total Number of word"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "### Example :- \n",
    "\n",
    "- We will again use the same vocabulary we had built in the Bag-of-Words model to show how to calculate the TF for Review #2:\n",
    "\n",
    "                                        Review 2: This movie is not scary and is slow\n",
    "\n",
    "Here,\n",
    "\n",
    "- Vocabulary: ‘This’, ‘movie’, ‘is’, ‘very’, ‘scary’, ‘and’, ‘long’, ‘not’,  ‘slow’, ‘spooky’,  ‘good’\n",
    "\n",
    "- Total Number of words in Review 2 = 8\n",
    "\n",
    "TF (‘this’) = (number of times ‘this’ appears in review 2)/(number of terms in review 2) = 1/8\n",
    "TF(‘movie’) = 1/8\n",
    "TF(‘is’) = 2/8 = 1/4\n",
    "TF(‘very’) = 0/8 = 0\n",
    "TF(‘scary’) = 1/8\n",
    "TF(‘and’) = 1/8\n",
    "TF(‘long’) = 0/8 = 0\n",
    "TF(‘not’) = 1/8\n",
    "TF(‘slow’) = 1/8\n",
    "TF( ‘spooky’) = 0/8 = 0\n",
    "TF(‘good’) = 0/8 = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"3.webp\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Inverse Document Frequency :-\n",
    "\n",
    "- The inverse document frequency is a measure of whether a term is rare or frequent across the documents in the entire corpus. It highlights those words which occur in very few documents across the corpus, or in simple language, the words that are rare have high IDF score.\n",
    "\n",
    "                             Formula - IDF =  log10(number of sentence / number of sentence containing the word)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We can calculate the IDF values for the all the words in Review 2:\n",
    "\n",
    "                                    Review 2: This movie is not scary and is slow\n",
    "\n",
    "IDF(‘this’) =  log(number of sentence/number of sentence containing the word ‘this’) = log(3/3) = log(1) = 0\n",
    "\n",
    "\n",
    "Similarly,\n",
    "\n",
    "IDF(‘movie’, ) = log(3/3) = 0\n",
    "IDF(‘is’) = log(3/3) = 0\n",
    "IDF(‘not’) = log(3/1) = log(3) = 0.48\n",
    "IDF(‘scary’) = log(3/2) = 0.18\n",
    "IDF(‘and’) = log(3/3) = 0\n",
    "IDF(‘slow’) = log(3/1) = 0.48"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"4.webp\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hence, we see that words like “is”, “this”, “and”, etc., are reduced to 0 and have little importance; while words like “scary”, “long”, “good”, etc. are words with more importance and thus have a higher value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can now compute the TF-IDF score for each word in the corpus. Words with a higher score are more important, and those with a lower score are less important:\n",
    "\n",
    "<img src=\"1.jpg\">"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We can now calculate the TF-IDF score for every word in Review 2:\n",
    "\n",
    "                                        Review 2: This movie is not scary and is slow\n",
    "\n",
    "TF-IDF(‘this’, Review 2) = TF(‘this’, Review 2) * IDF(‘this’) = 1/8 * 0 = 0\n",
    "\n",
    "Similarly,\n",
    "\n",
    "TF-IDF(‘movie’, Review 2) = 1/8 * 0 = 0\n",
    "TF-IDF(‘is’, Review 2) = 1/4 * 0 = 0\n",
    "TF-IDF(‘not’, Review 2) = 1/8 * 0.48 = 0.06\n",
    "TF-IDF(‘scary’, Review 2) = 1/8 * 0.18 = 0.023\n",
    "TF-IDF(‘and’, Review 2) = 1/8 * 0 = 0\n",
    "TF-IDF(‘slow’, Review 2) = 1/8 * 0.48 = 0.06"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"5.webp\">\n",
    "\n",
    "- We have now obtained the TF-IDF scores for our vocabulary. TF-IDF also gives larger values for less frequent words and is high when both IDF and TF values are high i.e the word is rare in all the documents combined but frequent in a single document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "paragraph =  \"\"\"  This movie is very scary and long.\n",
    "                 This movie is not scary and is slow.\n",
    "                 This movie is spooky and good.\n",
    "                 \"\"\"\n",
    "               \n",
    "               \n",
    "# Cleaning the texts\n",
    "import re\n",
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "corpus = []\n",
    "for i in range(len(sentences)):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', sentences[i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)\n",
    "\n",
    "    \n",
    "# Creating the TF-IDF model\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "cv = TfidfVectorizer()\n",
    "X = cv.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.29628336, 0.        , 0.29628336, 0.50165133, 0.29628336,\n",
       "        0.        , 0.38151877, 0.        , 0.        , 0.29628336,\n",
       "        0.50165133],\n",
       "       [0.26359985, 0.        , 0.5271997 , 0.        , 0.26359985,\n",
       "        0.44631334, 0.3394328 , 0.44631334, 0.        , 0.26359985,\n",
       "        0.        ],\n",
       "       [0.32052772, 0.54270061, 0.32052772, 0.        , 0.32052772,\n",
       "        0.        , 0.        , 0.        , 0.54270061, 0.32052772,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let me summarize what we’ve covered in the article:\n",
    "\n",
    "- Bag of Words just creates a set of vectors containing the count of word occurrences in the document (reviews), while the TF-IDF model contains information on the more important words and the less important ones as well.\n",
    "\n",
    "- Bag of Words vectors are easy to interpret. However, TF-IDF usually performs better in machine learning models.\n",
    "\n",
    "- While both Bag-of-Words and TF-IDF have been popular in their own regard, there still remained a void where understanding the context of words was concerned. Detecting the similarity between the words ‘spooky’ and ‘scary’, or translating our given documents into another language, requires a lot more information on the documents.\n",
    "\n",
    "- This is where Word Embedding techniques such as Word2Vec, Continuous Bag of Words (CBOW), Skipgram, etc. come in."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
