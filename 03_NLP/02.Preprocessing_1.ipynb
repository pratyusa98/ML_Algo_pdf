{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization,Stemming, Lemmatization, Stop Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tokenization\n",
    "\n",
    "- Tokenization is the process of tokenizing or splitting a string, text into a list of tokens. One can think of token as parts like a word is a token in a sentence, and a sentence is a token in a paragraph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code #1: Sentence Tokenization – Splitting sentences in the paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello Mr. Smith, how are you doing today.', '?']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize \n",
    "  \n",
    "text = \"Hello Mr. Smith, how are you doing today. ?\"\n",
    "sent_tokenize(text) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How sent_tokenize works ?\n",
    "The sent_tokenize function uses an instance of PunktSentenceTokenizer from the nltk.tokenize.punkt module, which is already been trained and thus very well knows to mark the end and beginning of sentence at what characters and punctuation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code #: Word Tokenization – Splitting words in a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', 'Mr.', 'Smith', ',', 'how', 'are', 'you', 'doing', 'today', '.', '?']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize \n",
    "  \n",
    "word_tokenize(text) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How word_tokenize works?\n",
    "word_tokenize() function is a wrapper function that calls tokenize() on an instance of the TreebankWordTokenizer class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Stemming\n",
    "\n",
    "- Refers to the process of slicing the end or the beginning of words with the intention of removing affixes (lexical additions to the root of the word).\n",
    "- Affixes that are attached at the beginning of the word are called prefixes (e.g. “astro” in the word “astrobiology”) and the ones attached at the end of the word are called suffixes (e.g. “ful” in the word “helpful”).\n",
    "\n",
    "- A stemming algorithm reduces the words “chocolates”, “chocolatey”, “choco” to the root word, “chocolate” and “retrieval”, “retrieved”, “retrieves” reduce to the stem “retrieve”. \n",
    "- Stemming is an important part of the pipelining process in Natural language processing. The input to the stemmer is tokenized words."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " Some more example of stemming for root word \"like\" include:\n",
    "->\"likes\"\n",
    "->\"liked\"\n",
    "->\"likely\"\n",
    "->\"liking\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Errors in Stemming: \n",
    "- There are mainly two errors in stemming – \n",
    " \n",
    "        1. over-stemming:- occurs when two words are stemmed from the same root that are of different stems. Over-stemming can also be regarded as false-positives. \n",
    "        2. under-stemming :- occurs when two words are stemmed from the same root that are not of different stems. Under-stemming can be interpreted as false-negatives. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some Stemming algorithms are: \n",
    "    1. Porter’s Stemmer algorithm \n",
    "\n",
    "Advantage: It produces the best output as compared to other stemmers and it has less error rate.\n",
    "\n",
    "Limitation:  Morphological variants produced are not always real words.\n",
    "\n",
    "    2. Snowball Stemmer:\n",
    "\n",
    "- When compared to the Porter Stemmer, the Snowball Stemmer can map non-English words too. \n",
    "- Since it supports other languages the Snowball Stemmers can be called a multi-lingual stemmer. \n",
    "- The Snowball stemmers are also imported from the nltk package. This stemmer is based on a programming language called ‘Snowball’ that processes small strings and is the most widely used stemmer.\n",
    "- The Snowball stemmer is way more aggressive than Porter Stemmer and is also referred to as Porter2 Stemmer. Because of the improvements added when compared to the Porter Stemmer, the Snowball stemmer is having greater computational speed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "program  :  program\n",
      "programs  :  program\n",
      "programer  :  program\n",
      "programing  :  program\n",
      "programers  :  program\n"
     ]
    }
   ],
   "source": [
    "# import these modules \n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "ps = PorterStemmer() \n",
    "\n",
    "# choose some words to be stemmed \n",
    "words = [\"program\", \"programs\", \"programer\", \"programing\", \"programers\"] \n",
    "\n",
    "for w in words: \n",
    "    print(w, \" : \", ps.stem(w)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Programers  :  program\n",
      "program  :  program\n",
      "with  :  with\n",
      "programing  :  program\n",
      "languages  :  languag\n"
     ]
    }
   ],
   "source": [
    "## Stemming words from sentences\n",
    "#importing modules \n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.tokenize import word_tokenize  #tokenization\n",
    "\n",
    "ps = PorterStemmer() \n",
    "\n",
    "sentence = \"Programers program with programing languages\"\n",
    "words = word_tokenize(sentence) \n",
    "\n",
    "for w in words: \n",
    "    print(w, \" : \", ps.stem(w)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SnowballStemmer \n",
      "\n",
      "cared  :  care\n",
      "university  :  univers\n",
      "fairly  :  fair\n",
      "easily  :  easili\n",
      "singing  :  sing\n",
      "sings  :  sing\n",
      "sung  :  sung\n",
      "singer  :  singer\n",
      "sportingly  :  sport\n",
      "\n",
      "PorterStemmer\n",
      "\n",
      "cared  :  care\n",
      "university  :  univers\n",
      "fairly  :  fairli\n",
      "easily  :  easili\n",
      "singing  :  sing\n",
      "sings  :  sing\n",
      "sung  :  sung\n",
      "singer  :  singer\n",
      "sportingly  :  sportingli\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import PorterStemmer \n",
    "\n",
    "#the stemmer requires a language parameter \n",
    "snow_stemmer = SnowballStemmer(language='english')\n",
    "porter_stem = PorterStemmer() \n",
    "\n",
    "#list of tokenized words \n",
    "words = ['cared','university','fairly','easily','singing','sings','sung','singer','sportingly'] \n",
    "\n",
    "#stem's of each word\n",
    "print('SnowballStemmer \\n')\n",
    "for i in words:\n",
    "    print(i, \" : \", snow_stemmer.stem(i))\n",
    "print('\\nPorterStemmer\\n')\n",
    "for j in words: \n",
    "    print(j, \" : \", porter_stem.stem(j)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Lemmatization\n",
    "\n",
    "- Lemmatization is the process of grouping together the different inflected forms of a word so they can be analysed as a single item. Lemmatization is similar to stemming but it brings context to the words. So it links words with similar meaning to one word.\n",
    "\n",
    "- Text preprocessing includes both Stemming as well as Lemmatization. Many times people find these two terms confusing. Some treat these two as same. Actually, lemmatization is preferred over Stemming because lemmatization does morphological analysis of the words."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Examples of lemmatization:\n",
    "\n",
    "-> rocks : rock\n",
    "-> corpora : corpus\n",
    "-> better : good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rocks  :  rock\n",
      "corpora  :  corpus\n",
      "better  :  better\n",
      "\n",
      "PorterStemmer\n",
      "\n",
      "rocks  :  rock\n",
      "corpora  :  corpora\n",
      "better  :  better\n"
     ]
    }
   ],
   "source": [
    "# import these modules \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "words = ['rocks','corpora','better'] \n",
    "\n",
    "for j in words: \n",
    "    print(j, \" : \", lemmatizer.lemmatize(j)) \n",
    "    \n",
    "print('\\nPorterStemmer\\n')\n",
    "\n",
    "for k in words: \n",
    "    print(k, \" : \", porter_stem.stem(k)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Different Types Lemmatization :- https://www.geeksforgeeks.org/python-lemmatization-approaches-with-examples/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.  Stop Words\n",
    "<img src=\"1.png\">\n",
    "\n",
    "- Stop Words: A stop word is a commonly used word (such as “the”, “a”, “an”, “in”) that a search engine has been programmed to ignore, both when indexing entries for searching and when retrieving them as the result of a search query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These Are All Stops Word That We Have To Remove\n",
      "\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "print(\"These Are All Stops Word That We Have To Remove\\n\")\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With Stop Word:  ['This', 'is', 'a', 'sample', 'sentence', ',', 'showing', 'off', 'the', 'stop', 'words', 'filtration', '.']\n",
      "After Remove Stops Word:  ['This', 'sample', 'sentence', ',', 'showing', 'stop', 'words', 'filtration', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "example_sent = \"\"\"This is a sample sentence, \n",
    "                    showing off the stop words filtration.\"\"\"\n",
    "\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "word_tokens = word_tokenize(example_sent) \n",
    "\n",
    "filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "\n",
    "filtered_sentence = [] \n",
    "\n",
    "for w in word_tokens: \n",
    "    if w not in stop_words: \n",
    "        filtered_sentence.append(w) \n",
    "\n",
    "print(\"With Stop Word: \",word_tokens) \n",
    "print(\"After Remove Stops Word: \",filtered_sentence) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now All Put It on to One Paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "paragraph = \"\"\"Thank you all so very much. Thank you to the Academy. \n",
    "            Thank you to all of you in this room. I have to congratulate \n",
    "               the other incredible nominees this year. The Revenant was \n",
    "               the product of the tireless efforts of an unbelievable cast\n",
    "               and crew. First off, to my brother in this endeavor, Mr. Tom \n",
    "               Hardy. Tom, your talent on screen can only be surpassed by \n",
    "               your friendship off screen … thank you for creating a t\n",
    "               ranscendent cinematic experience. Thank you to everybody at \n",
    "               Fox and New Regency … my entire team. I have to thank \n",
    "               everyone from the very onset of my career … To my parents; \n",
    "               none of this would be possible without you. And to my \n",
    "               friends, I love you dearly; you know who you are. And lastly,\n",
    "               I just want to say this: Making The Revenant was about\n",
    "               man's relationship to the natural world. A world that we\n",
    "               collectively felt in 2015 as the hottest year in recorded\n",
    "               history. Our production needed to move to the southern\n",
    "               tip of this planet just to be able to find snow. Climate\n",
    "               change is real, it is happening right now. It is the most\n",
    "               urgent threat facing our entire species, and we need to work\n",
    "               collectively together and stop procrastinating. We need to\n",
    "               support leaders around the world who do not speak for the \n",
    "               big polluters, but who speak for all of humanity, for the\n",
    "               indigenous people of the world, for the billions and \n",
    "               billions of underprivileged people out there who would be\n",
    "               most affected by this. For our children’s children, and \n",
    "               for those people out there whose voices have been drowned\n",
    "               out by the politics of greed. I thank you all for this \n",
    "               amazing award tonight. Let us not take this planet for \n",
    "               granted. I do not take tonight for granted. Thank you so very much.\"\"\"\n",
    "               \n",
    "               \n",
    "# text Preprocessing\n",
    "import re\n",
    "from nltk.corpus import stopwords #stopword\n",
    "from nltk.stem.porter import PorterStemmer #for Stemming\n",
    "from nltk.stem import WordNetLemmatizer #for lemmatization\n",
    "\n",
    "ps = PorterStemmer() #inialize stemmer\n",
    "wordnet=WordNetLemmatizer() #inialize lemmatize\n",
    "\n",
    "## You can use any one but lemmatize is better than stemmer\n",
    "\n",
    "#step- 1\n",
    "sentences = nltk.sent_tokenize(paragraph) #sentence Tokenize\n",
    "\n",
    "corpus = []\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    # step 2 using some preprocess like remove unnecessary word,lower the sentence etc\n",
    "    review = re.sub('[^a-zA-Z]', ' ', sentences[i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    \n",
    "    #step3 use stemmer and remove stopwords in sentence\n",
    "#     review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "    #step3 use lemmatize and remove stopwords in sentence Use any one of these.\n",
    "    \n",
    "    review = [wordnet.lemmatize(word) for word in review if word not in set(stopwords.words('english'))]\n",
    "\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['thank much',\n",
       " 'thank academy',\n",
       " 'thank room',\n",
       " 'congratulate incredible nominee year',\n",
       " 'revenant product tireless effort unbelievable cast crew',\n",
       " 'first brother endeavor mr tom hardy',\n",
       " 'tom talent screen surpassed friendship screen thank creating ranscendent cinematic experience',\n",
       " 'thank everybody fox new regency entire team',\n",
       " 'thank everyone onset career parent none would possible without',\n",
       " 'friend love dearly know',\n",
       " 'lastly want say making revenant man relationship natural world',\n",
       " 'world collectively felt hottest year recorded history',\n",
       " 'production needed move southern tip planet able find snow',\n",
       " 'climate change real happening right',\n",
       " 'urgent threat facing entire specie need work collectively together stop procrastinating',\n",
       " 'need support leader around world speak big polluter speak humanity indigenous people world billion billion underprivileged people would affected',\n",
       " 'child child people whose voice drowned politics greed',\n",
       " 'thank amazing award tonight',\n",
       " 'let u take planet granted',\n",
       " 'take tonight granted',\n",
       " 'thank much']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now Our Data Cleaned So We Go towards The text vectorization using Bag_Of_Words(Count Vectorizer) ,TFIDF, Unigram,BiGram,n-Gram,Word2vc and Average Word2vec"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
