{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Function\n",
    "\n",
    "- Activation function decides, whether a neuron should be activated or not by calculating weighted sum and further adding bias with it. The purpose of the activation function is to introduce non-linearity into the output of a neuron.\n",
    "\n",
    "### Explanation :-\n",
    "We know, neural network has neurons that work in correspondence of weight, bias and their respective activation function. In a neural network, we would update the weights and biases of the neurons on the basis of the error at the output. This process is known as back-propagation. Activation functions make the back-propagation possible since the gradients are supplied along with the error to update the weights and biases. <br>\n",
    "<b>Why do we need Non-linear activation functions :-</b><br>\n",
    "A neural network without an activation function is essentially just a linear regression model. The activation function does the non-linear transformation to the input making it capable to learn and perform more complex tasks.\n",
    "\n",
    "There are several type of Activation But here i dicuss some of them:\n",
    "            1. Sigmoid (Binary Classification)\n",
    "            2. Tanh\n",
    "            3. Relu\n",
    "            4. Leaky Relu\n",
    "            5. Linear\n",
    "            6. Softmax (Use Multiclass Classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Sigmoid\n",
    "\n",
    "- The Sigmoid Function curve looks like a S-shape.\n",
    "<img src=\"14.png\">\n",
    "\n",
    "- The main reason why we use sigmoid function is because it exists between (0 to 1). Therefore, it is especially used for models where we have to predict the probability as an output.Since probability of anything exists only between the range of 0 and 1, sigmoid is the right choice.\n",
    "\n",
    "#### Derivative Of Sigmoid Function\n",
    "<img src=\"1.jpeg\" style=\"height:250px\">\n",
    "\n",
    "- Sigmoid Function Derivative range from 0 to 0.25\n",
    "\n",
    "<b>Uses :</b> Usually used in <u>output layer</u> of a binary classification, where result is either 0 or 1, as value for sigmoid function lies between 0 and 1 only so, result can be predicted easily to be 1 if value is greater than 0.5 and 0 otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Tanh or hyperbolic tangent Activation Function\n",
    "\n",
    "- tanh is also like logistic sigmoid but better. The range of the tanh function is from (-1 to 1). tanh is also sigmoidal (s - shaped).\n",
    "<img src=\"15.png\" style=\"height:350px\">\n",
    "\n",
    "- The advantage is that the negative inputs will be mapped strongly negative and the zero inputs will be mapped near zero in the tanh graph.\n",
    "- The activation that works almost always better than sigmoid function is Tanh function also knows as Tangent Hyperbolic function. It’s actually mathematically shifted version of the sigmoid function. Both are similar and can be derived from each other.\n",
    "\n",
    "#### Derivative Of Tanh Function:-\n",
    "\n",
    "<img src=\"1.jpg\" style=\"height:350px\">\n",
    "\n",
    "Sigmoid Function Derivative range from 0 to 1\n",
    "\n",
    "<b>Uses :- </b>Usually used in <u>hidden layers</u> of a neural network as it’s values lies between -1 to 1 hence the mean for the hidden layer comes out be 0 or very close to it, hence helps in centering the data by bringing mean close to 0. This makes learning for the next layer much easier.\n",
    "\n",
    "<img src=\"18.png\" style=\"height:350px\">\n",
    "\n",
    "\n",
    "#### Point :-\n",
    "1. tanh and logistic sigmoid are the most popular activation functions in 90’s but because of their Vanishing gradient problem and sometimes Exploding gradient problem (because of weights), they aren’t mostly used now.\n",
    "2. These days Relu activation function is widely used. Even though, it sometimes gets into vanishing gradient problem, variants of Relu help solving such cases.\n",
    "3. tanh is preferred to sigmoid for faster convergence BUT again, this might change based on data. Data will also play an important role in deciding which activation function is best to choose."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
