{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Adaptive Gradient (AdaGrad)\n",
    "\n",
    "- Adaptive Gradient as the name suggests adopts the learning rate of parameters by updating it at each iteration depending on the position it is present, i.e- by adapting slower learning rates when features are occurring frequently and adapting higher learning rate when features are infrequent.\n",
    "- The motivation behind Adagrad is to have different learning rates for each neuron of each hidden layer for each iteration.\n",
    "\n",
    "##### But why do we need different learning rates?\n",
    "Data sets have two types of features:\n",
    "- Dense features, e.g. House Price Data set (Large number of non-zero valued features), where we should perform smaller updates on such features; and\n",
    "- Sparse Features, e.g. Bag of words (Large number of zero valued features), where we should perform larger updates on such features.\n",
    "\n",
    "It has been found that Adagrad greatly improved the robustness of SGD, and is used for training large-scale neural nets at Google.\n",
    "\n",
    "\n",
    "<img src=\"50.png\">\n",
    "<img src=\"51.png\">\n",
    "\n",
    "η : initial Learning rate\n",
    "\n",
    "ϵ : smoothing term that avoids division by zero\n",
    "\n",
    "w: Weight of parameters\n",
    "- In SGD learning Rate same for all weight but in Adagrad this is different for all.\n",
    "\n",
    "#### Advantage:\n",
    "- No need to update the learning rate manually as it changes adaptively with iterations.\n",
    "- If we have some Sparse and Dense feature it automatically takes out what learning rate is suitable.\n",
    "\n",
    "#### Disadvantage:\n",
    "- As the number of iteration becomes very large learning rate decreases to a very small number which leads to slow convergence.\n",
    "- Computationally expensive as a need to calculate the second order derivative.\n",
    "\n",
    "Adadelta, RMSProp, and adam tries to resolve Adagrad’s radically diminishing learning rates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. AdaDelta\n",
    "\n",
    "- It is simply an extension of AdaGrad that seeks to reduce its monotonically decreasing learning rate.\n",
    "- Instead of summing all the past gradients, AdaDelta restricts the no. of summation values to a limit (w).\n",
    "- In AdaDelta, the sum of past gradients (w) is defined as “Decaying Average of all past squared gradients”. The current average at the iteration then depends only on the previous average and current gradient.\n",
    "\n",
    "<img src=\"50.png\">\n",
    "<img src=\"52.png\">\n",
    "\n",
    "- Instead of inefficiently storing all previous squared gradients, we recursively define a decaying average of all past squared gradients. The running average at each time step then depends (as a fraction γ , similarly to the Momentum term) only on the previous average and the current gradient.\n",
    "\n",
    "#### Advantages:\n",
    "Now the learning rate does not decay and the training does not stop.\n",
    "#### Disadvantages:\n",
    "Computationally expensive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. RMSProp\n",
    "- RMSProp is Root Mean Square Propagation. It was devised by Geoffrey Hinton.\n",
    "- RMSProp tries to resolve Adagrad’s radically diminishing learning rates by using a moving average of the squared gradient. It utilizes the magnitude of the recent gradient descents to normalize the gradient.\n",
    "- In RMSProp learning rate gets adjusted automatically and it chooses a different learning rate for each parameter.\n",
    "- RMSProp divides the learning rate by the average of the exponential decay of squared gradients\n",
    "- Its cost function same as Adadelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Adam — Adaptive Moment Estimation\n",
    "\n",
    "- It is a combination of RMSProp and Momentum.\n",
    "- This method computes adaptive learning rate for each parameter.\n",
    "- In addition to storing the previous decaying average of squared gradients, it also holds the average of past gradient similar to Momentum. Thus, Adam behaves like a heavy ball with friction which prefers flat minima in error surface.\n",
    "- Another method that calculates the individual adaptive learning rate for each parameter from estimates of first (Momentum) and second (RMSProp) moments of the gradients.\n",
    "\n",
    "<img src=\"53.jpg\" style=\"height:500px\">\n",
    "\n",
    "#### Advantages:\n",
    "- The method is too fast and converges rapidly.\n",
    "- Rectifies vanishing learning rate, high variance.\n",
    "#### Disadvantages:\n",
    "- Computationally costly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
