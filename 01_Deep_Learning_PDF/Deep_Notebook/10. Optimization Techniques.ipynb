{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization Techniques\n",
    "\n",
    "- Optimization algorithms are responsible for reducing losses and provide most accurate results possible. \n",
    "- The weight is initialized using some initialization strategies and is updated with each epoch according to the equation. The best results are achieved using some optimization strategies or algorithms called Optimizer.\n",
    "\n",
    "\n",
    "- Some of the techniques that we will be discussing in this article is-\n",
    "\n",
    "                * Gradient Descent\n",
    "                * Stochastic Gradient Descent (SGD)\n",
    "                * Mini-Batch Stochastic Gradient Descent (MB — SGD)\n",
    "                * SGD with Momentum\n",
    "                * Nesterov Accelerated Gradient (NAG)\n",
    "                * Adaptive Gradient (AdaGrad)\n",
    "                * AdaDelta\n",
    "                * RMSProp\n",
    "                * Adam\n",
    "\n",
    "<img src=\"47.png\" style=\"height:300px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Gradient Descent or Batch Gradient Descent\n",
    "\n",
    "- A Gradient Descent is an iterative algorithm, that starts from a random point on the function and traverses down its slope in steps until it reaches lowest point (global minima) of that function. \n",
    "- This algorithm is apt for cases where optimal points cannot be found by equating the slope of the function to 0. For the function to reach minimum value, the weights should be altered. \n",
    "- With the help of back propagation, loss is transferred from one layer to another and “weights” parameter are also modified depending on loss so that loss can be minimized.\n",
    "\n",
    "### Point :\n",
    "<p style=\"color:red\"><b><u>1. Use all training Sample for a forward pass and adjust the weights. </u></b></p>\n",
    "2. This makes it computationally intensive. \n",
    "3. Another drawback is there are chances the iteration values may get stuck at local minima or saddle point and never converge to minima. To obtain the best solution, the must reach global minima.\n",
    "4. Good For Small training data.\n",
    "\n",
    "Cost function: θ=θ−α⋅∇J(θ)\n",
    "\n",
    "\n",
    "\n",
    "#### Advantages:\n",
    "- Easy computation.\n",
    "- Easy to implement.\n",
    "- Easy to understand.\n",
    "\n",
    "#### Disadvantages:\n",
    "- May trap at local minima.\n",
    "- Weights are changed after calculating gradient on the whole dataset. So, if the dataset is too large than this may take years to converge to the minima.\n",
    "- Requires large memory to calculate gradient on the whole dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Stochastic Gradient Descent\n",
    "\n",
    "- Stochastic Gradient Descent is an extension of Gradient Descent, where it overcomes some of the disadvantages of Gradient Descent algorithm. \n",
    "- SGD tries to overcome the disadvantage of computationally intensive by computing the derivative of one point at a time. \n",
    "- Due to this fact, SGD takes more number of iterations compared to GD to reach minimum and also contains some noise when compared to Gradient Descent. \n",
    "- As SGD computes derivatives of only 1 point at a time, the time taken to complete one epoch is large compared to Gradient Descent algorithm.\n",
    "\n",
    "\n",
    "### Point :\n",
    "<p style=\"color:red\"><b><u>1. Use One (Randomly Picked) Sample for a forward pass and adjust the weights.</u></b></p>\n",
    "2. Good when training set is very big and we dont want too much computation.\n",
    "\n",
    "\n",
    "cost function θ=θ−α⋅∇J(θ;x(i);y(i)) , where {x(i) ,y(i)} are the training examples.\n",
    "\n",
    "#### Advantages:\n",
    "- Frequent updates of model parameters hence, converges in less time.\n",
    "- Requires less memory as no need to store values of loss functions.\n",
    "- May get new minima’s.\n",
    "\n",
    "#### Disadvantages:\n",
    "- High variance(noisey) in model parameters.\n",
    "- May shoot even after achieving global minima.\n",
    "- To get the same convergence as gradient descent needs to slowly reduce the value of learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Mini Batch — Stochastic Gradient Descent\n",
    "\n",
    "- MB-SGD is an extension of SGD algorithm. It overcomes the time-consuming complexity of SGD by taking a batch of points / subset of points from dataset to compute derivative.\n",
    "- It’s best among all the variations of gradient descent algorithms. It is an improvement on both SGD and standard gradient descent. It updates the model parameters after every batch. So, the dataset is divided into various batches and after every batch, the parameters are updated.\n",
    "- This is a mixture of both stochastic and batch gradient descent.\n",
    "- The training set is divided into multiple groups called batches. Each batch has a number of training samples in it. \n",
    "- At a time a single batch is passed through the network which computes the loss of every sample in the batch and uses their average to update the parameters of the neural network. \n",
    "- For example, say the training set has 100 training examples which is divided into 5 batches with each batch containing 20 training examples. This means that the equation in figure2 will be iterated over 5 times (number of batches).\n",
    "\n",
    "\n",
    "### Point:\n",
    "<p style=\"color:red\"><b><u>1. Use a Batch Of (Randomly Picked) Sample for a forward pass and adjust the weights.</u></b></p>\n",
    "2. It is observed that the derivative of loss function of MB-SGD is similar to the loss function of GD after some iterations. But the number iterations to achieve minima in MB-SGD is large compared to GD and is computationally expensive. The update of weights in much noisier because the derivative is not always towards minima.\n",
    "\n",
    "θ=θ−α⋅∇J(θ; B(i)), where {B(i)} are the batches of training examples.\n",
    "\n",
    "#### Advantages:\n",
    "- Frequently updates the model parameters and also has less variance.\n",
    "- Requires medium amount of memory.\n",
    "- Easily fits in the memory\n",
    "- It is computationally efficient\n",
    "- Benefit from vectorization\n",
    "- If stuck in local minimums, some noisy steps can lead the way out of them\n",
    "- Average of the training samples produces stable error gradients and convergence\n",
    "\n",
    "!!!! This ensures the following advantages of both stochastic and batch gradient descent are used due to which Mini Batch Gradient Descent is most commonly used in practice.\n",
    "\n",
    "#### See How in this above three convergence Occure towards minima point \n",
    "\n",
    "<img src=\"48.png\" style=\"height:300px\">\n",
    "\n",
    "Here We see in SGD Due to frequent updates the steps taken towards the minima are very noisy. This can often lead the gradient descent into other directions. Also, due to noisy steps it may take longer to achieve convergence to the minima of the loss function. to reduce this we can use SGD with Momentum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. SGD with Momentum\n",
    "\n",
    "- Momentum was invented for reducing high variance in SGD and softens the convergence. \n",
    "- It accelerates the convergence towards the relevant direction and reduces the fluctuation to the irrelevant direction. One more hyperparameter is used in this method known as momentum symbolized by ‘γ’(gamma).\n",
    "- It is an adaptive optimization algorithm which exponentially uses weighted average gradients over previous iterations to stabilize the convergence, resulting in quicker optimization. \n",
    "- This is done by adding a fraction (gamma) to the previous iteration values. \n",
    "- Essentially the momentum term increase when the gradient points are in the same directions and reduce when gradients fluctuate. As a result, the value of loss function converges faster than expected.\n",
    "\n",
    "<img src=\"56.png\" style=\"height:80px\">\n",
    "\n",
    "#### Advantages:\n",
    "- Reduces the oscillations and high variance of the parameters.\n",
    "- Converges faster than gradient descent.\n",
    "\n",
    "#### Disadvantages:\n",
    "- One more hyper-parameter is added which needs to be selected manually and accurately.\n",
    "\n",
    "<img src=\"4.jpg\" style=\"height:250px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Nesterov accelerated gradient(NAG)\n",
    "\n",
    "<img src=\"2.jpeg\">\n",
    "\n",
    "- Momentum may be a good method but if the momentum is too high the algorithm may miss the local minima and may continue to rise up. So, to resolve this issue the NAG algorithm was developed.\n",
    "- Nesterov accelerated gradient (NAG) is a way to give momentum more precision.\n",
    "- The idea of the NAG algorithm is very similar to SGD with momentum with a slight variant. In the case of SGD with momentum algorithm, the momentum and gradient are computed on previous updated weight.\n",
    "- Both NAG and SGD with momentum algorithms work equally well and share the same advantages and disadvantages.\n",
    "\n",
    "\n",
    "<img src=\"57.png\" style=\"height:80px\">\n",
    "\n",
    "<img src=\"49.png\" style=\"height:350px\">\n",
    "\n",
    "#### figure (a) :\n",
    "- In figure (a), update 1 is positive i.e., the gradient is negative because as w_0 increases L decreases. Even update 2 is positive as well and you can see that the update is slightly larger than update 1 because of momentum. \n",
    "- By now, you should be convinced that update 3 will be bigger than both update 1 and 2 simply because of momentum and the positive update history. \n",
    "- Update 4 is where things get interesting. In SGD with Momentum case, due to the positive history, the update overshoots and the descent recovers by doing negative updates.\n",
    "\n",
    "#### figure (b) :\n",
    "- But in NAG’s case, every update happens in two steps — first, a partial update, where we get to the look_ahead point and then the final update (see the NAG update rule), see figure (b). \n",
    "- First 3 updates of NAG are pretty similar to the momentum-based method as both the updates (partial and final) are positive in those cases. But the real difference becomes apparent during update 4. \n",
    "- As usual, each update happens in two stages, the partial update (4a) is positive, but the final update (4b) would be negative as the calculated gradient at w_lookahead would be negative (convince yourself by observing the graph). \n",
    "- This negative final update slightly reduces the overall magnitude of the update, still resulting in an overshoot but a smaller one when compared to the vanilla momentum-based gradient descent. And that my friend, is how NAG helps us in reducing the overshoots, i.e. making us take shorter U-turns.\n",
    "\n",
    "\n",
    "#### Advantages:\n",
    "- Does not miss the local minima.\n",
    "- Slows if minima’s are occurring.\n",
    "#### Disadvantages:\n",
    "- Still, the hyperparameter needs to be selected manually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Point :\n",
    "\n",
    "- By using NAG technique, we are now able to adapt error function with the help of previous and future values and thus eventually speed up the convergence. Now, in the next techniques we will try to adapt alter or vary the individual parameters depending on the importance factor it plays in each case."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
