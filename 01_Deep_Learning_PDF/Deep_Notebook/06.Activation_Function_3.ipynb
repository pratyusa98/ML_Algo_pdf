{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax\n",
    "\n",
    "- Softmax is used as the activation function for multi-class classification tasks, usually the last layer. \n",
    "- We talked about its role transforming numbers (aka logits) into probabilities that sum to one.\n",
    "- Let’s not forget it is also an activation function which means it helps our model achieve non-linearity. Linear combinations of linear combinations will always be linear but adding activation function helps gives our model ability to handle non-linear data.\n",
    "- Output of other activation functions such as sigmoid does not necessarily sum to one. Having outputs summing to one makes softmax function great for probability analysis.\n",
    "- The function is great for classification problems, especially if you’re dealing with multi-class classification problems, as it will report back the “confidence score” for each class. Since we’re dealing with probabilities here, the scores returned by the softmax function will add up to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical representation\n",
    "\n",
    "<img src=\"1.svg\">\n",
    "\n",
    "Where:\n",
    "\n",
    "σ = softmax\n",
    "\n",
    "<img src=\"2.svg\" style=\"float:left\">=input vector\n",
    "\n",
    "\n",
    "<img src=\"3.svg\" style=\"float:left\">=standard exponential function for input vector\n",
    "\n",
    "K\t=\tnumber of classes in the multi-class classifier\n",
    "\n",
    "<img src=\"4.svg\" style=\"float:left\">=standard exponential function for output vector\n",
    "\n",
    "- It states that we need to apply a standard exponential function to each element of the output layer, and then normalize these values by dividing by the sum of all the exponentials. Doing so ensures the sum of all exponentiated values adds up to 1.\n",
    "\n",
    "### Here are the steps For Softmax:\n",
    "1. Exponentiate every element of the output layer and sum the results\n",
    "2. Take each element of the output layer, exponentiate it and divide by the sum obtained in step 1\n",
    "\n",
    "### Example Implementation\n",
    "\n",
    "<img src=\"27.png\" style=\"width:450px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To start, let’s declare an array which imitates the output layer of a neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2. , 1. , 0.1])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## as we know softmax used in output layer so here i take a outputlayer value\n",
    "import numpy as np\n",
    "output_layer = np.array([2.0,1.0,0.1])\n",
    "output_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By step 1 we need to exponentiate each of the elements of the output layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7.3890561 , 2.71828183, 1.10517092])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exponentiated = np.exp(output_layer)\n",
    "exponentiated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### According to step 2  calculate probabilities! We can use Numpy to divide each element by exponentiated sum and store results in another array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.65900114 0.24243297 0.09856589]\n",
      "1.0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "probabilities = exponentiated / np.sum(exponentiated)\n",
    "print(probabilities)\n",
    "print(sum(probabilities))\n",
    "print(np.argmax(probabilities))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Here see the output are formed. If we sum three then we get probability 1. after this you use argmax function which return highest value index number. See here return 0 as 0 index have 0.65 value which is highest among three value.\n",
    "\n",
    "##### When you use softmax in your dataset you should use argmax function to predict output.\n",
    "\n",
    "- From a probabilistic perspective, if the argmax() function returns 1 in the large value, it returns 0 for the other two array indexes,here it giving full weight to index 0 and no weight to index 1 and index 2 for the largest value in the list [0.65,0.24,0.09]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In the Keras deep learning library with a three-class classification task, use of softmax in the output layer may look as follows:\n",
    "<br>\n",
    "<h3> <center>model.add(Dense(no.of output layer, activation='softmax'))</center></h3>\n",
    "- It apply when you have multiclass problem aries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Differences between Sigmoid and Softmax Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "### sigmoid function\n",
    "def sigmoid(x):\n",
    "    s = 1 / (1 + np.exp(-x))\n",
    "    return s\n",
    "\n",
    "## softmax function\n",
    "def softmax(x):\n",
    "    exponentiated = np.exp(x)\n",
    "    probabilities = exponentiated / np.sum(exponentiated)\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Sigmoid---\n",
      "[0.38 0.77 0.48 0.92]\n",
      "2.54\n",
      "**************************************************\n",
      "---Softmax---\n",
      "[0.04 0.21 0.06 0.7 ]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "x = np.array([-0.5, 1.2, -0.1, 2.4])\n",
    "a = sigmoid(x)\n",
    "print(\"--- Sigmoid---\")\n",
    "print(a.round(2))\n",
    "print(sum(a).round(2))\n",
    "\n",
    "print(50*\"*\")\n",
    "\n",
    "output_layer = np.array([-0.5, 1.2, -0.1, 2.4])\n",
    "b = softmax(output_layer)\n",
    "print(\"---Softmax---\")\n",
    "print(b.round(2))\n",
    "print(sum(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The key takeaway from this example is:\n",
    "* <b>Sigmoid:</b> probabilities produced by a Sigmoid are independent. Furthermore, they are not constrained to sum to one: 0.38 + 0.77 + 0.48 + 0.92 = 2.54. The reason for this is because the Sigmoid looks at each raw output value separately.\n",
    "\n",
    "* <b>Softmax:</b> the outputs are interrelated. The Softmax probabilities will always sum to one by design: 0.04 + 0.21 + 0.06 + 0.7 = 1.00. In this case, if we want to increase the likelihood of one class, the other has to decrease by an equal amount."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary..\n",
    "\n",
    "#### Characteristics of a Sigmoid Activation Function:\n",
    "\n",
    "1. Used for Binary Classification in the Logistic Regression model\n",
    "2. The probabilities sum does not need to be 1\n",
    "3. Used as an Activation Function while building a Neural Network\n",
    "\n",
    "#### Characteristics of a Softmax Activation Function\n",
    "1. Used for Multi-classification in the Logistics Regression model\n",
    "2. The probabilities sum will be 1\n",
    "3. Used in the different layers of Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Function For Regression Problem\n",
    " \n",
    "- Linear Activation Function:-\n",
    "<center><h4>Equation : f(x) = x </h4>\n",
    "    <h5>Range : (-infinity to infinity)</h5></center>\n",
    "<img src=\"29.png\" style=\"height:250px\">\n",
    "\n",
    "- No matter how many layers we have, if all are linear in nature, the final activation function of last layer is nothing but just a linear function of the input of first layer.\n",
    "- Linear activation function is used at just one place i.e. output layer.\n",
    "-  If we will differentiate linear function to bring non-linearity, result will no more depend on input “x” and function will become constant, it won’t introduce any ground-breaking behavior to our algorithm.\n",
    "\n",
    "<b>Uses:</b> Calculation of price of a house is a regression problem. House price may have any big/small value, so we can apply linear activation at output layer. Even in this case neural net must have any non-linear function at hidden layers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
