{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weight Initialization\n",
    "\n",
    "- The weight initialization technique you choose for your neural network can determine how quickly the network converges or whether it converges at all. Although the initial values of these weights are just one parameter among many to tune, they are incredibly important. Their distribution affects the gradients and, therefore, the effectiveness of training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why is weight initialization important?\n",
    "\n",
    "- Improperly initialized weights can negatively affect the training process by contributing to the vanishing or exploding gradient problem.\n",
    "- With the vanishing gradient problem, the weight update is minor and results in slower convergence — this makes the optimization of the loss function slow and in a worst case scenario, may stop the network from converging altogether.\n",
    "- Conversely, initializing with weights that are too large may result in exploding gradient values during forward propagation or back-propagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Zero initialization :\n",
    "- If all the weights are initialized with 0, the derivative with respect to loss function is the same for every weight(w), thus all weights have the same value in subsequent iterations. \n",
    "- This makes hidden units symmetric and continues for all the n iterations i.e. setting weights to 0 does not make it better than a linear model. \n",
    "- An important thing to keep in mind is that biases have no effect what so ever when initialized with 0.\n",
    "- It also  gives problems like vanishing gradient problem.\n",
    "\n",
    "\n",
    "### 2.  initialization With -ve Number :\n",
    "\n",
    "- If all weigth can be negative then it affect Relu Activation Function.As in -ve Relu comes under dead activation problem. so we cant use this technique.\n",
    "- Weights can’t be too high as gives problems like exploding Gradient problem(weights of the model explode to infinity), which means that a large space is made available to search for global minima hence convergence becomes slow.\n",
    "\n",
    "##### To prevent the gradients of the network’s activations from vanishing or exploding, we need to have following rules:\n",
    "            1. The mean of the activations should be zero.\n",
    "            2. The variance of the activations should stay the same across every layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Idea 1 : Normal or Naïve Initialization:\n",
    "\n",
    "-  In normal distribution weights can be a part of normal or gaussian distribution with mean as zero and a unit standard deviation.\n",
    "<img src=\"41.png\">\n",
    "- Random initialization is done so that convergence is not to a false minima.\n",
    "- In Keras it can be simply written as hyperparameter as - kernel_initializer='random_normal'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Idea 2: Uniform Initialization:\n",
    "\n",
    "-  In uniform initialization of weights , weights belong to a uniform distribution in range a,b with values of a and b as below:\n",
    "<img src=\"42.png\">\n",
    "- Whenever <b> sigmoid </b> activation function is used as , Uniform works well.\n",
    "- In Keras it can be simply written as hyperparameter as - kernel_initializer='random_uniform'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Idea 3: Xavier/ Glorot Weight Initialization:\n",
    "\n",
    "- The variance of weights in the case normal distribution was not taken care of which resulted in too large or too small activation values which again led to exploding gradient and vanishing gradient problems respectively, when back propagation was done.\n",
    "- In order to overcome this problem Xavier Initialization was introduced. It keeps the variance the same across every layer. We will assume that our layer’s activations are normally distributed around zero.\n",
    "- Glorot or Xavier had a belief that if they maintain variance of activations in all the layers going forward and backward convergence will be fast as compared to using standard initialization where gap was larger.\n",
    "- It have Two Varient\n",
    "\n",
    "            a. Normal Distribution - kernel_initializer='glorot_normal'\n",
    "            b. Uniform Distribution - kernel_initializer='glorot_uniform'\n",
    "            \n",
    "<b>Point :</b> Works well with <b> tanh , sigmoid </b>activation functions.\n",
    "\n",
    "### a. Normal Distribution:\n",
    "- In Normal Distribution, weights belong to normal distribution where mean is zero and standard deviation is as below:\n",
    "<img src=\"43.png\" style=\"height:150px\">\n",
    "\n",
    "### b. Uniform Distribution:\n",
    "- Uniform Distribution , weights belong to uniform distribution in range of a and b defined as below:\n",
    "<img src=\"44.png\" style=\"height:200px\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Idea 4: He-Initialization:\n",
    "- When using activation functions that were zero centered and have output range between-1,1 for activation functions like tanh and softsign, activation outputs were having mean of 0 and standard deviation around 1 average wise.\n",
    "- But if ReLu is used instead of tanh, it was observed that on average it has standard deviation very close to square root of 2 divided by input connections.\n",
    " - It have Two Varient\n",
    "\n",
    "            a. Normal Distribution - kernel_initializer='he_normal'\n",
    "            b. Uniform Distribution - kernel_initializer='he_uniform'\n",
    "\n",
    "<b>Point :</b> Works well with <b>Relu And Leaky Relu </b> activation functions.\n",
    "### a. Normal Distribution:\n",
    "- In He-Normal initialization method, weights belong to normal distribution where mean is zero and standard deviation is as below:\n",
    "<img src=\"45.png\" style=\"height:150px\">\n",
    "\n",
    "### b. Uniform Initialization :\n",
    "- In He Uniform Initialization weights belong to uniform distribution in range as shown below:\n",
    "<img src=\"46.png\" style=\"height:150px\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
