{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Loss Functions\n",
    "\n",
    "- The loss function is the function that computes the distance between the current output of the algorithm and the expected output. It’s a method to evaluate how your algorithm models the data. It can be categorized into two groups. One for classification (discrete values, 0,1,2…) and the other for regression (continuous values)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TYPES OF LOSS FUNCTION:\n",
    "\n",
    "1. Regression Loss Functions\n",
    "\n",
    "            Mean Absolute Error\n",
    "            Mean Squared Error\n",
    "            Root Mean Square error (RMSE)\n",
    "\n",
    "2. Binary Classification Loss Functions\n",
    "\n",
    "            Binary Cross-Entropy\n",
    "         \n",
    "3. Multi-Class Classification Loss Functions\n",
    "\n",
    "            Multi-Class Cross-Entropy Loss\n",
    "            Sparse Multiclass Cross-Entropy Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We know all of this regression loss function but here i discuss a brief\n",
    "\n",
    "### Mean Absolute Error\n",
    "\n",
    "- Regression metric which measures the average magnitude of errors in a group of predictions, without considering their directions. In other words, it’s a mean of absolute differences among predictions and expected results where all individual deviations have even importance.\n",
    "\n",
    "<img src=\"36.png\">\n",
    "\n",
    "where:\n",
    "\n",
    "i — index of sample,\n",
    "\n",
    "ŷ — predicted value,\n",
    "\n",
    "y — expected value,\n",
    "\n",
    "m — number of samples in dataset.\n",
    "\n",
    "Sometimes it is possible to see the form of formula with swapped predicted value and expected value, but it works the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Squared Error\n",
    "\n",
    "- One of the most commonly used and firstly explained regression metrics. Average squared difference between the predictions and expected results. In other words, an alteration of MAE where instead of taking the absolute value of differences, they are squared.\n",
    "\n",
    "- In MAE, the partial error values were equal to the distances between points in the coordinate system. Regarding MSE, each partial error is equivalent to the area of the square created out of the geometrical distance between the measured points. All region areas are summed up and averaged.\n",
    "\n",
    "<img src=\"379.png\">\n",
    "\n",
    "Where\n",
    "\n",
    "i — index of sample,\n",
    "\n",
    "ŷ — predicted value,\n",
    "\n",
    "y — expected value,\n",
    "\n",
    "m — number of samples in dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Root Mean Square error (RMSE)\n",
    "\n",
    "- Root Mean Square error is the extension of MSE — measured as the average of square root of sum of squared differences between predictions and actual observations.\n",
    "<img src=\"1.jfif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Losses\n",
    "\n",
    "### Binary Classification Loss Functions\n",
    "\n",
    "### Binary Cross Entropy\n",
    "\n",
    "- Also called Sigmoid Cross-Entropy loss. It is a Sigmoid activation plus a Cross-Entropy loss. Unlike Softmax loss it is independent for each vector component (class), meaning that the loss computed for every CNN output vector component is not affected by other component values.\n",
    "\n",
    "- Binary cross entropy measures how far away from the true value (which is either 0 or 1) the prediction is for each of the classes and then averages these class-wise errors to obtain the final loss.\n",
    "\n",
    "- We can define cross entropy as the difference between two probability distributions p and q, where p is our true output and q is our estimate of this true output.\n",
    "\n",
    "- it Only use for binary classification problem\n",
    "<img src=\"2.jfif\" style=\"height:250px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Class Classification Loss Functions\n",
    "\n",
    "### Categorical cross-entropy\n",
    "\n",
    "- Used binary and multiclass problem, the label needs to be encoded as categorical, one-hot encoding representation (for 3 classes: [0, 1, 0], [1,0,0]…)\n",
    "- It is a loss function that is used for single label categorization. This is when only one category is applicable for each data point. In other words, an example can belong to one class only.\n",
    "\n",
    "<img src=\"37.png\">\n",
    "\n",
    "- Use categorical crossentropy in classification problems where only one result can be correct.\n",
    "- Example:​ In the ​MNIST​​ problem where you have images of the numbers 0,1, 2, 3, 4, 5, 6, 7, 8, and 9. Categorical crossentropy gives the probability that an image of a number is, for example, a 4 or a 9.\n",
    "\n",
    "- Categorical cross-entropy will compare the distribution of the predictions (the activations in the output layer, one for each class) with the true distribution, where the probability of the true class is set to 1 and 0 for the other classes. To put it in a different way, the true class is represented as a one-hot encoded vector, and the closer the model’s outputs are to that vector, the lower the loss.\n",
    "\n",
    "### Sparse Categorical cross-entropy\n",
    "\n",
    "- Used binary and multiclass problem (the label is an integer — 0 or 1 or … n, depends on the number of labels)\n",
    "\n",
    "<img src=\"38.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All Losses : https://keras.io/api/losses/\n",
    "### Summary\n",
    "\n",
    "There are three kinds of classification tasks:\n",
    "\n",
    "        1. Binary classification: two exclusive classes\n",
    "        2. Multi-class classification: more than two exclusive classes\n",
    "        3. Multi-label classification: just non-exclusive classes\n",
    "        \n",
    "Here, we can say\n",
    "\n",
    "        1. In the case of (1), you need to use binary cross entropy.\n",
    "        2. In the case of (2), you need to use categorical cross entropy.\n",
    "        3.In the case of (3), you need to use binary cross entropy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
