{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q. What is the Problem With Simple RNN ?\n",
    "\n",
    "- Ans:- Lets take Many-Many (Same Length)\n",
    "\n",
    "<img src=\"83.png\" style=\"height:350px\">\n",
    "\n",
    "- Here yi4 value depends a lot on xi4 and 03 and less depends on xi1 and 01 which is a limitation of this simple RNN because in real world application yi4 may depends more on xi1 and 01 and less xi4 and 03. this is called long term dependency.\n",
    "\n",
    "- Simple RNN Cant handle long term dependency. \n",
    "\n",
    "- long term dependency - Later output depends a lot on earlier input i.e. ( yi4 -> xi1 )\n",
    "\n",
    "<img src=\"84.png\" style=\"height:350px\">\n",
    "\n",
    "- So that small change in yi4 should create similar change in weight of xi1 result vanish the gradient. in simple RNN with Sigmoid and tanh later output nodes of network are less sensitive to the  input this is happen due to vanishing gradient problem.\n",
    "\n",
    "\n",
    "- If a sequence is long enough, they’ll have a hard time carrying information from earlier time steps to later ones. So if you are trying to process a paragraph of text to do predictions, RNN’s may leave out important information from the beginning.\n",
    "\n",
    "- During back propagation, recurrent neural networks suffer from the vanishing gradient problem. Gradients are values used to update a neural networks weights. The vanishing gradient problem is when the gradient shrinks as it back propagates through time. If a gradient value becomes extremely small, it doesn’t contribute too much learning.\n",
    "\n",
    "- So in recurrent neural networks, layers that get a small gradient update stops learning. Those are usually the earlier layers. So because these layers don’t learn, RNN’s can forget what it seen in longer sequences, thus having a short-term memory. If you want to know more about the mechanics of recurrent neural networks in general, you can read my previous post here.\n",
    "\n",
    "\n",
    "\n",
    "- To overcome this we use LSTM (Long Short Term Memory) And GRU (Gated Recurrent Unit)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  LSTM (Long Short Term Memory) \n",
    "\n",
    "- It takes care both long and short term dependency.\n",
    "\n",
    "<img src=\"88.png\" style=\"height:350px\">\n",
    "\n",
    "1. Input gate- It discover which value from input should be used to modify the memory. Sigmoid function decides which values to let through 0 or 1. And tanh function gives weightage to the values which are passed, deciding their level of importance ranging from -1 to 1. <br><br><br>\n",
    "\n",
    "2. Forget gate- It discover the details to be discarded from the block. A sigmoid function decides it. It looks at the previous state (ht-1) and the content input (Xt) and outputs a number between 0(omit this) and 1(keep this) for each number in the cell state Ct-1.<br><br><br>\n",
    "\n",
    "3. Output gate- The input and the memory of the block are used to decide the output. Sigmoid function decides which values to let through 0 or 1. And tanh function decides which values to let through 0, 1. And tanh function gives weightage to the values which are passed, deciding their level of importance ranging from -1 to 1 and multiplied with an output of sigmoid.\n",
    "\n",
    "<img src=\"89.png\" style=\"height:400px\">\n",
    "\n",
    "- It represents a full RNN cell that takes the current input of the sequence xi, and outputs the current hidden state, hi, passing this to the next RNN cell for our input sequence. The inside of an LSTM cell is a lot more complicated than a traditional RNN cell, while the conventional RNN cell has a single \"internal layer\" acting on the current state (ht-1) and input (xt)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-by-Step LSTM Walk Through\n",
    "\n",
    "- It takes care both long and short term dependency.\n",
    "<img src=\"86.png\" style=\"height:250px\">\n",
    "<img src=\"87.png\" style=\"height:90px\">\n",
    "\n",
    "#### Step-1 (forget gate layer)\n",
    "- The first step in our LSTM is to decide what information we’re going to throw away from the cell state. This decision is made by a sigmoid layer called the “forget gate layer.”\n",
    "- It looks at ht−1 and xt, and outputs a number between 0 and 1 for each number in the cell state Ct−1. A 1 represents “completely keep this” while a 0 represents “completely get rid of this.”\n",
    "\n",
    "<img src=\"90.png\" style=\"height:250px\">\n",
    "\n",
    "#### Step-2 (input gate layer)\n",
    "- In this step is to decide what new information we’re going to store in the cell state. \n",
    "- This has two parts. First, a sigmoid layer called the “input gate layer” decides which values we’ll update. Next, a tanh layer creates a vector of new candidate values, C~t, that could be added to the state. In the next step, we’ll combine these two to create an update to the state.\n",
    "\n",
    "<img src=\"91.png\" style=\"height:250px\">\n",
    "\n",
    "#### Step-3 (input gate layer)\n",
    "- It’s now time to update the old cell state, Ct−1, into the new cell state Ct. The previous steps already decided what to do, we just need to actually do it.\n",
    "\n",
    "- We multiply the old state by ft, forgetting the things we decided to forget earlier. Then we add it∗C~t. This is the new candidate values, scaled by how much we decided to update each state value.\n",
    "\n",
    "<img src=\"92.png\" style=\"height:250px\">\n",
    "\n",
    "#### Step-4 (output gate layer)\n",
    "- Finally, we need to decide what we’re going to output.\n",
    "- This output will be based on our cell state, but will be a filtered version. First, we run a sigmoid layer which decides what parts of the cell state we’re going to output. Then, we put the cell state through tanh (to push the values to be between −1 and 1) and multiply it by the output of the sigmoid gate, so that we only output the parts we decided to.\n",
    "\n",
    "<img src=\"93.png\" style=\"height:250px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU (Gated Recurrent Units)\n",
    "\n",
    "- So now we know how an LSTM work, let’s briefly look at the GRU. The GRU is the newer generation of Recurrent Neural networks and is pretty similar to an LSTM. GRU’s got rid of the cell state and used the hidden state to transfer information. It also only has two gates, a reset gate and update gate.\n",
    "\n",
    "<img src=\"94.png\" style=\"height:300px\">\n",
    "\n",
    "1. Update Gate - The update gate acts similar to the forget and input gate of an LSTM. It decides what information to throw away and what new information to add.\n",
    "2. Reset Gate- The reset gate is another gate is used to decide how much past information to forget.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- GRU’s has fewer tensor operations; therefore, they are a little speedier to train then LSTM’s. There isn’t a clear winner which one is better. Researchers and engineers usually try both to determine which one works better for their use case.\n",
    "\n",
    "\n",
    "<img src=\"95.png\" style=\"height:200px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"96.png\" >"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
