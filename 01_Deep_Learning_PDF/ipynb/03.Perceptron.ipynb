{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is  Perceptron?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Perceptron is a single layer neural network and a multi-layer perceptron is called Neural Networks.</h4> <br><br>\n",
    "1. Single-layered perceptron model <br>\n",
    "2. Multi-layered perceptron model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Single-layered perceptron model\n",
    "\n",
    "<img src=\"12.png\">\n",
    "\n",
    "- If you talk about the functioning of the single-layered perceptron model, its algorithm doesn’t have previous information, so initially, weights are allocated inconstantly, then the algorithm adds up all the weighted inputs, \n",
    "- if the added value is more than some pre-determined value( or, threshold value) then single-layered perceptron is stated as activated and delivered output as +1.\n",
    "- In simple words, multiple input values feed up to the perceptron model, model executes with input values, and if the estimated value is the same as the required output, then the model performance is found out to be satisfied, therefore weights demand no changes. In fact, if the model doesn’t meet the required result then few changes are made up in weights to minimize errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Multi-layered perceptron model\n",
    "<img src=\"13.png\">\n",
    "\n",
    "- In the forward stage, activation functions are originated from the input layer to the output layer, and in the backward stage, the error between the actual observed value and demanded given value is originated backward in the output layer for modifying weights and bias values.\n",
    "- In simple terms, multi-layered perceptron can be treated as a network of numerous artificial neurons overhead varied layers, the activation function is no longer linear, instead, non-linear activation functions such as Sigmoid functions, TanH, ReLU activation Functions, etc are deployed for execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
