{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Regularization Techniques\n",
    "\n",
    "- One of the most common problem data science professionals face is to avoid overfitting. Have you come across a situation where your model performed exceptionally well on train data, but was not able to predict test data.\n",
    "\n",
    "<img src=\"1.webp\">\n",
    "\n",
    "\n",
    "- Have you seen this image before? As we move towards the right in this image, our model tries to learn too well the details and the noise from the training data, which ultimately results in poor performance on the unseen data.\n",
    "- In other words, while going towards the right, the complexity of the model increases such that the training error reduces but the testing error doesn’t. This is shown in the image below.\n",
    "\n",
    "<img src=\"2.webp\" style=\"height:440px\">\n",
    "\n",
    "- If you’ve built a neural network before, you know how complex they are. This makes them more prone to overfitting. Regularization is a technique which makes slight modifications to the learning algorithm such that the model generalizes better. This in turn improves the model’s performance on the unseen data as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different Regularization Techniques in Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. L1 & L2 regularization\n",
    "\n",
    "- L1 and L2 are the most common types of regularization. These update the general cost function by adding another term known as the regularization term.\n",
    "\n",
    "                            Cost function = Loss (say, binary cross entropy) + Regularization term\n",
    "\n",
    "- Due to the addition of this regularization term, the values of weight matrices decrease because it assumes that a neural network with smaller weight matrices leads to simpler models. Therefore, it will also reduce overfitting to quite an extent."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Below is the sample code to apply L2 regularization to a Dense layer.\n",
    "\n",
    "from keras import regularizers\n",
    "model.add(Dense(64, input_dim=64,\n",
    "                kernel_regularizer=regularizers.l2(0.01)\n",
    "                \n",
    "\n",
    "## Below is the sample code to apply L1 regularization to a Dense layer.\n",
    "\n",
    "from keras import regularizers\n",
    "model.add(Dense(64, input_dim=64,\n",
    "                kernel_regularizer=regularizers.l1(0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: Here the value 0.01 is the value of regularization parameter, i.e., lambda, which we need to optimize further. We can optimize it using the hyper parameter tuning method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dropout\n",
    "\n",
    "- This is the one of the most interesting types of regularization techniques. It also produces very good results and is consequently the most frequently used regularization technique in the field of deep learning.\n",
    "\n",
    "- To understand dropout, let’s say our neural network structure is akin to the one shown below:\n",
    "\n",
    "<img src=\"59.png\">\n",
    "\n",
    "- So what does dropout do? At every iteration, it randomly selects some nodes and removes them along with all of their incoming and outgoing connections as shown below. So each iteration has a different set of nodes and this results in a different set of outputs. It can also be thought of as an ensemble technique in machine learning.\n",
    "\n",
    "#### Point:-\n",
    "- dropout is usually preferred when we have a large neural network structure in order to introduce more randomness."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## In keras, we can implement dropout using the keras core layer. Below is the python code for it:\n",
    "\n",
    "from keras.layers.core import Dropout\n",
    "\n",
    "model = Sequential([\n",
    " Dense(output_dim=hidden1_num_units, input_dim=input_num_units, activation='relu'),\n",
    " Dropout(0.25),\n",
    "\n",
    "Dense(output_dim=output_num_units, input_dim=hidden5_num_units, activation='softmax'),\n",
    " ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note : As you can see, we have defined 0.25 as the probability of dropping. We can tune it further for better results using the Hyper parameter tuning method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Augmentation (For Image Data)\n",
    "\n",
    "- The simplest way to reduce overfitting is to increase the size of the training data. In machine learning, we were not able to increase the size of training data as the labeled data was too costly.\n",
    "\n",
    "-  But, now let’s consider we are dealing with images. In this case, there are a few ways of increasing the size of the training data – rotating the image, flipping, scaling, shifting, etc. In the below image, some transformation has been done on the handwritten digits dataset.\n",
    "\n",
    "- This technique is known as data augmentation. This usually provides a big leap in improving the accuracy of the model. It can be considered as a mandatory trick in order to improve our predictions.\n",
    "\n",
    "- In keras, we can perform all of these transformations using ImageDataGenerator. It has a big list of arguments which you you can use to pre-process your training data.\n",
    "\n",
    "<img src=\"60.png\">"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Below is the sample code to implement it.\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "datagen = ImageDataGenerator(horizontal flip=True)\n",
    "datagen.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Batch Normalization\n",
    "- Batch normalization is a technique for improving the speed, performance, and stability of artificial neural networks, also known as batch norm. The idea is to normalize the inputs of each layer in such a way that, they have a mean activation output zero and a unit standard deviation.\n",
    "\n",
    "- The reason for the ‘batch’ in the term Batch Normalization is because neural networks are usually trained with a collated set of data at a time, this set or group of data is referred to as a batch. The operation within the BN technique occurs to an entire batch of input values as opposed to a single input value.\n",
    "\n",
    "#### Why should we normalize the input?\n",
    "- Let say we have 2D data, X1, and X2. X1 feature has a very wider spread between 200 to -200 whereas the X2 feature has a very narrow spread. The left graph shows the variance of the data which has different ranges. The right graph shows data lies between -2 to 2 and it’s normally distributed with 0 mean and unit variance.\n",
    "\n",
    "<img src=\"61.png\">\n",
    "\n",
    "- Essentially, scaling the inputs through normalization gives the error surface a more spherical shape, where it would otherwise be a very high curvature ellipse. Having an error surface with high curvature will mean that we take many steps that aren’t necessarily in the optimal direction. \n",
    "- When we scale the inputs, we reduce the curvature, which makes methods that ignore curvature like gradient descent work much better. When the error surface is circular or spherical, the gradient points right at the minimum.\n",
    "\n",
    "<img src=\"62.png\" style=\"height:300px\">\n",
    "\n",
    "### Benefits of Batch Normalization\n",
    "- Inclusion of Batch Normalization technique in deep neural networks improves training time\n",
    "- BN enables the utilization of larger learning rates, this shortness the time of convergence when training neural networks\n",
    "- Reduces the common problem of vanishing gradients\n",
    "- Covariate shift within neural network is reduced\n",
    "\n",
    "#### Point: In Batch normalization just as we standardize the inputs, the same way we standardize the activation at all the layers so that, at each layer we have 0 mean and unit standard deviation."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## In keras, we can implement BatchNormalization using the keras layer. Below is the python code for it:\n",
    "model = Sequential([\n",
    " Dense(output_dim=hidden1_num_units, input_dim=input_num_units, activation='relu'),\n",
    " keras.layers.BatchNormalization(),\n",
    "\n",
    "Dense(output_dim=output_num_units, input_dim=hidden5_num_units, activation='softmax'),\n",
    " ])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
